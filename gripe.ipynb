{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gripe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biblioteca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coleta e tratamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z3/2x5_0s3j31zdf3c9g85v3rbc0000gn/T/ipykernel_21832/152469830.py:1: DtypeWarning: Columns (15,59,61,62,63,91,93,105,107,114,116,117,118,122,143) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dados_2021 = pd.read_csv('dados/gripe_2021.csv', sep=';', encoding='ISO-8859-1')\n",
      "/var/folders/z3/2x5_0s3j31zdf3c9g85v3rbc0000gn/T/ipykernel_21832/152469830.py:2: DtypeWarning: Columns (15,53,59,61,62,63,91,93,120,122,143,171) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dados_2022 = pd.read_csv('dados/gripe_2022.csv', sep=';', encoding='ISO-8859-1')\n",
      "/var/folders/z3/2x5_0s3j31zdf3c9g85v3rbc0000gn/T/ipykernel_21832/152469830.py:3: DtypeWarning: Columns (15,18,20,23,53,59,61,63,72,74,79,91,93,109,130,175,178) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dados_2023 = pd.read_csv('dados/gripe_2023.csv', sep=';', encoding='ISO-8859-1')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dados_2021 = pd.read_csv('dados/gripe_2021.csv', sep=';', encoding='ISO-8859-1')\n",
    "dados_2022 = pd.read_csv('dados/gripe_2022.csv', sep=';', encoding='ISO-8859-1')\n",
    "dados_2023 = pd.read_csv('dados/gripe_2023.csv', sep=';', encoding='ISO-8859-1')\n",
    "\n",
    "dados = pd.concat([dados_2021, dados_2022, dados_2023])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DT_NOTIFIC', 'SEM_NOT', 'DT_SIN_PRI', 'SEM_PRI', 'SG_UF_NOT',\n",
       "       'ID_REGIONA', 'CO_REGIONA', 'ID_MUNICIP', 'CO_MUN_NOT', 'ID_UNIDADE',\n",
       "       ...\n",
       "       'VG_ENC', 'VG_REINF', 'REINF', 'FAB_ADIC', 'LOT_RE_BI', 'FAB_RE_BI',\n",
       "       'DOSE_ADIC', 'DOS_RE_BI', 'LOTE_ADIC', 'TABAG'],\n",
       "      dtype='object', length=190)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DT_NOTIFIC          0\n",
       "SEM_NOT             0\n",
       "DT_SIN_PRI          0\n",
       "SEM_PRI             0\n",
       "SG_UF_NOT           0\n",
       "               ...   \n",
       "FAB_RE_BI     2545509\n",
       "DOSE_ADIC     2561067\n",
       "DOS_RE_BI     2545495\n",
       "LOTE_ADIC     2561129\n",
       "TABAG         2566070\n",
       "Length: 190, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificando nulos e inválidos\n",
    "dados.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenando os dados\n",
    "dados = pd.concat([dados_2021, dados_2022, dados_2023])\n",
    "\n",
    "# Mantendo apenas as colunas relevantes\n",
    "dados = dados[['DT_NOTIFIC', 'SG_UF_NOT', 'CO_REGIONA', 'CO_MUN_NOT', 'CS_SEXO', 'DT_NASC', 'NU_IDADE_N', 'CS_GESTANT', 'CS_RACA']]\n",
    "\n",
    "# Ajustando a conversão de datas\n",
    "dados['DT_NOTIFIC'] = pd.to_datetime(dados['DT_NOTIFIC'], format='%d/%m/%Y')\n",
    "dados['DT_NASC'] = pd.to_datetime(dados['DT_NASC'], format='%d/%m/%Y', errors='coerce')  # usando errors='coerce' para lidar com qualquer dado incorreto\n",
    "\n",
    "# Criando metadados\n",
    "dados['ANO'] = dados['DT_NOTIFIC'].dt.year\n",
    "dados['MES'] = dados['DT_NOTIFIC'].dt.month\n",
    "\n",
    "# Convertendo variáveis categóricas em numéricas conforme necessário\n",
    "dados = pd.get_dummies(dados, columns=['SG_UF_NOT', 'CS_SEXO', 'CS_GESTANT', 'CS_RACA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT_NOTIFIC      0\n",
      "CO_REGIONA      0\n",
      "CO_MUN_NOT      0\n",
      "DT_NASC         0\n",
      "NU_IDADE_N      0\n",
      "ANO             0\n",
      "MES             0\n",
      "SG_UF_NOT_AC    0\n",
      "SG_UF_NOT_AL    0\n",
      "SG_UF_NOT_AM    0\n",
      "SG_UF_NOT_AP    0\n",
      "SG_UF_NOT_BA    0\n",
      "SG_UF_NOT_CE    0\n",
      "SG_UF_NOT_DF    0\n",
      "SG_UF_NOT_ES    0\n",
      "SG_UF_NOT_GO    0\n",
      "SG_UF_NOT_MA    0\n",
      "SG_UF_NOT_MG    0\n",
      "SG_UF_NOT_MS    0\n",
      "SG_UF_NOT_MT    0\n",
      "SG_UF_NOT_PA    0\n",
      "SG_UF_NOT_PB    0\n",
      "SG_UF_NOT_PE    0\n",
      "SG_UF_NOT_PI    0\n",
      "SG_UF_NOT_PR    0\n",
      "SG_UF_NOT_RJ    0\n",
      "SG_UF_NOT_RN    0\n",
      "SG_UF_NOT_RO    0\n",
      "SG_UF_NOT_RR    0\n",
      "SG_UF_NOT_RS    0\n",
      "SG_UF_NOT_SC    0\n",
      "SG_UF_NOT_SE    0\n",
      "SG_UF_NOT_SP    0\n",
      "SG_UF_NOT_TO    0\n",
      "CS_SEXO_1       0\n",
      "CS_SEXO_F       0\n",
      "CS_SEXO_I       0\n",
      "CS_SEXO_M       0\n",
      "CS_GESTANT_0    0\n",
      "CS_GESTANT_1    0\n",
      "CS_GESTANT_2    0\n",
      "CS_GESTANT_3    0\n",
      "CS_GESTANT_4    0\n",
      "CS_GESTANT_5    0\n",
      "CS_GESTANT_6    0\n",
      "CS_GESTANT_9    0\n",
      "CS_RACA_1.0     0\n",
      "CS_RACA_2.0     0\n",
      "CS_RACA_3.0     0\n",
      "CS_RACA_4.0     0\n",
      "CS_RACA_5.0     0\n",
      "CS_RACA_9.0     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Removendo linhas onde CO_REGIONA é nulo\n",
    "dados = dados.dropna(subset=['CO_REGIONA'])\n",
    "dados = dados.dropna(subset=['DT_NASC'])\n",
    "print(dados.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separando previsores e alvo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previsores salvos em ./variaveis/previsores.pickle\n",
      "Alvo salvo em ./variaveis/alvo.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dados_agregados = dados.groupby(['ANO', 'MES', 'CO_REGIONA']).size().reset_index(name='CASOS')\n",
    "previsores = dados_agregados.drop('CASOS', axis=1)\n",
    "alvo = dados_agregados['CASOS']\n",
    "previsores_arquivo = os.path.join('./variaveis', 'previsores.pickle')\n",
    "with open(previsores_arquivo, 'wb') as f:\n",
    "    pickle.dump(previsores, f)\n",
    "print(f'Previsores salvos em {previsores_arquivo}')\n",
    "\n",
    "alvo_arquivo = os.path.join('./variaveis', 'alvo.pickle')\n",
    "with open(alvo_arquivo, 'wb') as f:\n",
    "    pickle.dump(alvo, f)\n",
    "print(f'Alvo salvo em {alvo_arquivo}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escalonando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "previsores = scaler.fit_transform(previsores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisão Treino Teste 70 - 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previsores carregados de ./variaveis/previsores.pickle\n",
      "Alvo carregado de ./variaveis/alvo.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "previsores_arquivo = './variaveis/previsores.pickle'\n",
    "with open(previsores_arquivo, 'rb') as f:\n",
    "    previsores = pickle.load(f)\n",
    "print(f'Previsores carregados de {previsores_arquivo}')\n",
    "\n",
    "alvo_arquivo = './variaveis/alvo.pickle'\n",
    "with open(alvo_arquivo, 'rb') as f:\n",
    "    alvo = pickle.load(f)\n",
    "print(f'Alvo carregado de {alvo_arquivo}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(previsores, alvo, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop algoritimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Acurácia: 0.04554455445544554\n",
      "KNN F1 Score: 0.0016662131455732396\n",
      "KNN Matriz de Confusão:\n",
      "[[75 18  7 ...  0  0  0]\n",
      " [39 18  5 ...  0  0  0]\n",
      " [28 14 11 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rafaelmunareto/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Acurácia Média de Validação Cruzada: 0.04498508995576536\n",
      "\n",
      "Modelo KNN salvo em ./modelos/KNN.pickle\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rafaelmunareto/anaconda3/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rafaelmunareto/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Acurácia: 0.03498349834983498\n",
      "MLP F1 Score: 0.0003312170921924841\n",
      "MLP Matriz de Confusão:\n",
      "[[58 61 10 ...  0  0  0]\n",
      " [37 42  5 ...  0  0  0]\n",
      " [31 51  5 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rafaelmunareto/anaconda3/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rafaelmunareto/anaconda3/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rafaelmunareto/anaconda3/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rafaelmunareto/anaconda3/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Acurácia Média de Validação Cruzada: 0.0226351388551048\n",
      "\n",
      "Modelo MLP salvo em ./modelos/MLP.pickle\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rafaelmunareto/anaconda3/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelos = {\n",
    "    #'RandomForest': RandomForestClassifier(),\n",
    "    #'GradientBoosting': GradientBoostingClassifier(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'MLP': MLPClassifier(max_iter=50) \n",
    "}\n",
    "\n",
    "for nome, modelo in modelos.items():\n",
    "    modelo.fit(X_train, y_train)\n",
    "    predicoes = modelo.predict(X_test)\n",
    "    \n",
    "    acuracia = accuracy_score(y_test, predicoes)\n",
    "    f1 = f1_score(y_test, predicoes, average='macro')\n",
    "    matriz_confusao = confusion_matrix(y_test, predicoes)\n",
    "    \n",
    "    print(f'{nome} Acurácia: {acuracia}')\n",
    "    print(f'{nome} F1 Score: {f1}')\n",
    "    print(f'{nome} Matriz de Confusão:\\n{matriz_confusao}')\n",
    "    \n",
    "    # Validação cruzada para acurácia\n",
    "    scores = cross_val_score(modelo, X_train, y_train, scoring='accuracy', cv=5)\n",
    "    print(f'{nome} Acurácia Média de Validação Cruzada: {scores.mean()}\\n')\n",
    "    \n",
    "    # Salvar o modelo em arquivo pickle\n",
    "    modelo_arquivo = os.path.join('./modelos', f'{nome}.pickle')\n",
    "    with open(modelo_arquivo, 'wb') as f:\n",
    "        pickle.dump(modelo, f)\n",
    "    print(f'Modelo {nome} salvo em {modelo_arquivo}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação de hibirdos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo RandomForest carregado de ./modelos/RandomForest.pickle\n",
      "Modelo MLP carregado de ./modelos/MLP.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rafaelmunareto/anaconda3/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo Híbrido Acurácia: 0.02145214521452145\n",
      "Modelo Híbrido F1 Score: 0.0011988465934886997\n",
      "Modelo Híbrido Matriz de Confusão:\n",
      "[[26 12  7 ...  0  0  0]\n",
      " [10  9  2 ...  0  0  0]\n",
      " [10  2  3 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n",
      "Modelo híbrido salvo em ./modelos/ModeloHibrido.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Caminhos dos arquivos dos modelos salvos\n",
    "algo_1_path = './modelos/RandomForest.pickle'\n",
    "algo_2_path = './modelos/MLP.pickle'\n",
    "\n",
    "# Carregando os modelos previamente treinados\n",
    "with open(algo_1_path, 'rb') as f:\n",
    "    algo_1 = pickle.load(f)\n",
    "print(f'Modelo RandomForest carregado de {algo_1_path}')\n",
    "\n",
    "with open(algo_2_path, 'rb') as f:\n",
    "    algo_2 = pickle.load(f)\n",
    "print(f'Modelo MLP carregado de {algo_2_path}')\n",
    "\n",
    "# Criando um modelo de votação híbrido usando os classificadores carregados\n",
    "voting_clf = VotingClassifier(estimators=[('rf', algo_1), ('gb', algo_2)], voting='soft')\n",
    "voting_clf.fit(X_train, y_train)  # Treinando o classificador de votação com os dados\n",
    "\n",
    "# Previsões e métricas para o modelo híbrido\n",
    "pred_hibrido = voting_clf.predict(X_test)\n",
    "\n",
    "# Calculando métricas para o modelo híbrido\n",
    "acuracia_hibrido = accuracy_score(y_test, pred_hibrido)\n",
    "f1_hibrido = f1_score(y_test, pred_hibrido, average='macro')\n",
    "matriz_confusao_hibrido = confusion_matrix(y_test, pred_hibrido)\n",
    "\n",
    "print(f'Modelo Híbrido Acurácia: {acuracia_hibrido}')\n",
    "print(f'Modelo Híbrido F1 Score: {f1_hibrido}')\n",
    "print(f'Modelo Híbrido Matriz de Confusão:\\n{matriz_confusao_hibrido}')\n",
    "\n",
    "# Salvando o modelo híbrido em um arquivo pickle\n",
    "modelo_hibrido_arquivo = os.path.join('./modelos', 'ModeloHibrido.pickle')\n",
    "with open(modelo_hibrido_arquivo, 'wb') as f:\n",
    "    pickle.dump(voting_clf, f)\n",
    "print(f'Modelo híbrido salvo em {modelo_hibrido_arquivo}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae6500f62de4caa84f24758f8ff5a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=2024, description='Ano:', max=2024, min=2019)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e1b4c56c4c34f5c8f0ffff246540f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Mês:', index=1, options=(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12), value=2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f7512c0e7349a4add81f149f7c552f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Região:', options=(1331.0, 1332.0, 1333.0, 1334.0, 1335.0, 1336.0, 1337.0, 1338.0, 1339.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc043fbfac714a3aaa99abc092912beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Realizar Predição', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "032c4c2d39244e4a9f413ae522656309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ano_widget = widgets.IntSlider(value=2024, min=2019, max=2024, step=1, description='Ano:')\n",
    "mes_widget = widgets.Dropdown(options=list(range(1, 13)), value=2, description='Mês:')\n",
    "regioes_disponiveis = sorted(dados['CO_REGIONA'].unique())\n",
    "regiao_widget = widgets.Dropdown(options=regioes_disponiveis, value=regioes_disponiveis[0], description='Região:')\n",
    "\n",
    "display(ano_widget, mes_widget, regiao_widget)\n",
    "\n",
    "# Função para realizar predições\n",
    "# Removendo a referência ao OneHotEncoder e ajustando a predição\n",
    "def realizar_predicoes(ANO, MES, REGIAO):\n",
    "    predicoes_modelos = []\n",
    "    diretorio_modelos = './modelos'\n",
    "\n",
    "    # Ajustando para o uso sem necessidade de aplicação do OneHotEncoder novamente\n",
    "    for arquivo_modelo in os.listdir(diretorio_modelos):\n",
    "        caminho_completo = os.path.join(diretorio_modelos, arquivo_modelo)\n",
    "        \n",
    "        with open(caminho_completo, 'rb') as f:\n",
    "            modelo = pickle.load(f)\n",
    "\n",
    "        # Supondo que 'REGIAO' seja numérica e esteja disponível diretamente sem a necessidade de dummies\n",
    "        entrada_predicao = pd.DataFrame({'ANO': [ANO], 'MES': [MES], 'REGIAO': [REGIAO]})\n",
    "        predicao = modelo.predict(entrada_predicao)\n",
    "        predicoes_modelos.append(predicao[0])\n",
    "    \n",
    "    pred_media = np.mean(predicoes_modelos)\n",
    "    print(f\"\\nPredição média de casos para a região {REGIAO} no mês {MES} de {ANO}: {pred_media}\")\n",
    "\n",
    "    # Comparação com ano anterior\n",
    "    try:\n",
    "        casos_ano_anterior = dados[(dados['ANO'] == ANO - 1) & (dados['MES'] == MES) & (dados['REGIAO'] == REGIAO)]['CASOS'].values[0]\n",
    "        aumento_percentual = ((pred_media - casos_ano_anterior) / casos_ano_anterior) * 100\n",
    "        print(f\"Aumento percentual de casos em relação ao mesmo período do ano anterior: {aumento_percentual}%\")\n",
    "    except IndexError:\n",
    "        print(\"Dados para o ano anterior não encontrados.\")\n",
    "\n",
    "\n",
    "# Botão para realizar predições\n",
    "predicao_botao = widgets.Button(description=\"Realizar Predição\")\n",
    "predicao_output = widgets.Output()\n",
    "\n",
    "def on_predicao_botao_clicked(b):\n",
    "    with predicao_output:\n",
    "        print(\"Calculando predições...\")\n",
    "        realizar_predicoes(ano_widget.value, mes_widget.value, regiao_widget.value)\n",
    "\n",
    "predicao_botao.on_click(on_predicao_botao_clicked)\n",
    "\n",
    "display(predicao_botao, predicao_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
